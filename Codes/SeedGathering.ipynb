{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805e9d4c-f390-4aba-9ec8-627b9ed85aea",
   "metadata": {},
   "source": [
    "### SEED GATHERING GET CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a752eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++++++++++++++ Run this first time if you haven't installed from requirements.txt file/cloned the repo+++++++++++++++++++++\n",
    "# !pip install tree-sitter==0.20.4\n",
    "# !git clone https://github.com/tree-sitter/tree-sitter-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd3e144-73cb-42fa-9550-075c51686a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter_parser import LANGUAGE, make_parser, node_to_string\n",
    "import datasets\n",
    "import os\n",
    "import signal\n",
    "from multiprocessing import Pool\n",
    "#import os\n",
    "import boto3\n",
    "import smart_open\n",
    "#from datasets import load_dataset,Dataset\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "def download_contents(blob_id, src_encoding):\n",
    "    s3_url = f\"s3://softwareheritage/content/{blob_id}\"\n",
    "    with smart_open.open(s3_url, \"rb\", compression=\".gz\", transport_params={\"client\": s3}) as fin:\n",
    "        content = fin.read().decode(src_encoding)\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8ee925-5001-4a30-b29f-5741f8d173a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPLEVEL_DOC_COMMENT_QUERY = LANGUAGE.query(\"\"\"\n",
    "(\n",
    "  (function_definition\n",
    "    declarator: (function_declarator\n",
    "      declarator: (identifier) @fn-name\n",
    "    )\n",
    "    body: (compound_statement\n",
    "      (comment) @doc.comment\n",
    "    )\n",
    "  ) @function.def\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "'''\n",
    "def get_fns_with_docstrings(src, tree):\n",
    "    captures = TOPLEVEL_DOC_COMMENT_QUERY.captures(tree.root_node)\n",
    "    res = []\n",
    "    for capture in captures:\n",
    "        node, ty = capture\n",
    "        if ty != \"function.def\":\n",
    "            continue\n",
    "        # if the starting col is not 0, then it's not a top-level fn\n",
    "        _, col = node.start_point\n",
    "        if col != 0:\n",
    "            continue\n",
    "        res.append(node_to_string(src, node))\n",
    "    return res\n",
    "'''\n",
    "\n",
    "def get_fns_with_docstrings(src, tree):\n",
    "    captures = TOPLEVEL_DOC_COMMENT_QUERY.captures(tree.root_node)\n",
    "    res = []\n",
    "    current = {\"function_node\": None, \"name\": None, \"doc\": None}\n",
    "\n",
    "    for node, capture_name in captures:\n",
    "        if capture_name == \"fn-name\":\n",
    "            current[\"name\"] = node_to_string(src, node)\n",
    "\n",
    "        elif capture_name == \"doc.comment\":\n",
    "            current[\"doc\"] = node_to_string(src, node)\n",
    "\n",
    "        elif capture_name == \"function.def\":\n",
    "            current[\"function_node\"] = node\n",
    "\n",
    "            # Build the result once we have everything\n",
    "            if current[\"name\"] and current[\"doc\"]:\n",
    "                full_func_text = node_to_string(src, current[\"function_node\"])\n",
    "                res.append({\n",
    "                    \"function_name\": current[\"name\"],\n",
    "                    \"docstring\": current[\"doc\"],\n",
    "                    \"code\": full_func_text\n",
    "                })\n",
    "\n",
    "            # Reset for next\n",
    "            current = {\"function_node\": None, \"name\": None, \"doc\": None}\n",
    "\n",
    "    return res\n",
    "\n",
    "def parse_ex(parser, ex):\n",
    "    #ex = ex[\"content\"]\n",
    "    ex = download_contents(ex[\"blob_id\"], ex[\"src_encoding\"])\n",
    "    try:\n",
    "        buf = bytes(ex, \"utf8\")\n",
    "        tree = parser.parse(buf)\n",
    "        return get_fns_with_docstrings(buf, tree)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "# if one parser segfaults, we can just make a new one and other parsers will still be fine\n",
    "# WE LOVE TREE SITTER!\n",
    "PARSERS = None\n",
    "\n",
    "\n",
    "def process_chunk(idx_and_chunk):\n",
    "    assert PARSERS is not None\n",
    "    idx, chunk = idx_and_chunk\n",
    "    parser = PARSERS[idx]\n",
    "    chunk_new_funs = set()\n",
    "    for ex in chunk:\n",
    "        chunk_new_funs.update(parse_ex(parser, ex))\n",
    "    return chunk_new_funs\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    global PARSERS\n",
    "    ds = datasets.load_dataset(\n",
    "        args.dataset,\n",
    "        data_dir=args.data_dir,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    funs = set()\n",
    "    PARSERS = [make_parser() for _ in range(args.num_workers)]\n",
    "    total_len = len(ds)\n",
    "    CHUNK_SIZE = 1000 * args.num_workers\n",
    "\n",
    "    print(f\"Total length: {total_len}\")\n",
    "    print(f\"Chunk size: {CHUNK_SIZE}\")\n",
    "\n",
    "    chunk = []\n",
    "    p = Pool(args.num_workers)\n",
    "    for i, ex in enumerate(ds):\n",
    "        if i % (total_len // 100) == 0:\n",
    "            print(f\"{i}/{total_len}\")\n",
    "        try:\n",
    "            chunk.append(ex)\n",
    "            if len(chunk) == CHUNK_SIZE or i == total_len - 1:\n",
    "                print(f\"Processing chunk {i // CHUNK_SIZE}\")\n",
    "                # divide the chunk into NUM_WORKERS chunks\n",
    "                subchunk_size = len(chunk) // args.num_workers\n",
    "                subchunks = [chunk[i:i + subchunk_size]\n",
    "                             for i in range(0, len(chunk), subchunk_size)]\n",
    "                new_funs_iter = p.imap(\n",
    "                    process_chunk, [(i, subchunk) for i, subchunk in enumerate(subchunks)])\n",
    "                print(\"Getting new functions\")\n",
    "                len_before = len(funs)\n",
    "                while True:\n",
    "                    try:\n",
    "                        def timeout_handler(_, __):\n",
    "                            raise KeyboardInterrupt  # it's fineeeeeee\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(60)\n",
    "                        funs.update(next(new_funs_iter))\n",
    "                        signal.alarm(0)\n",
    "                    except KeyboardInterrupt:\n",
    "                        signal.alarm(0)\n",
    "                        print(\"Keyboard interrupt. Terminating pool\")\n",
    "                        p.terminate()\n",
    "                        p = Pool(args.num_workers)\n",
    "                        break\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                signal.alarm(0)\n",
    "\n",
    "                PARSERS = [make_parser() for _ in range(args.num_workers)]\n",
    "\n",
    "                print(\n",
    "                    f\"Done processing chunk {i // CHUNK_SIZE}. Got {len(funs) - len_before} new functions\")\n",
    "\n",
    "                chunk = []\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            chunk = []\n",
    "\n",
    "        if i == total_len - 1:\n",
    "            break\n",
    "\n",
    "    p.close()\n",
    "\n",
    "    new_ds_dict = {\n",
    "        \"content\": list(funs),\n",
    "        \"id\": list(range(len(funs)))\n",
    "    }\n",
    "\n",
    "    new_ds = datasets.Dataset.from_dict(new_ds_dict)\n",
    "    #new_ds.push_to_hub(args.push, private=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74accea3-de2a-4b38-bbf2-b0c7f2be7aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMWORKERS = os.cpu_count()\n",
    "NUMWORKERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8798ed1-24c7-4694-a97a-a381ab122392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d50b75f42c42eda9a1ba1ff8748c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"bigcode/the-stack-v2-dedup\", \"C++\", cache_dir=f\"./cache/stack\", streaming=False, split=\"train[:250]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6095d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import islice\n",
    "\n",
    "# small_subset = islice(ds, 50)\n",
    "\n",
    "# # Convert to list if you want to materialize it (use with caution, as this loads into memory)\n",
    "# ds = list(small_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17b23a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted functions in 0 and doc-comments:\n",
      "Extracted functions in 1 and doc-comments:\n",
      "Extracted functions in 2 and doc-comments:\n",
      "Extracted functions in 3 and doc-comments:\n",
      "Extracted functions in 4 and doc-comments:\n"
     ]
    }
   ],
   "source": [
    "# Setup a single Parser\n",
    "funs = set()\n",
    "\n",
    "parser = make_parser()\n",
    "\n",
    "for i in range(5):\n",
    "# Take one example manually\n",
    "    ex = ds[i]  # First example (directly)\n",
    "\n",
    "    # Download content if needed\n",
    "    content = download_contents(ex[\"blob_id\"], ex[\"src_encoding\"])\n",
    "\n",
    "    # Parse\n",
    "    src = bytes(content, \"utf8\")\n",
    "    tree = parser.parse(src)\n",
    "\n",
    "    # Extract functions\n",
    "    functions = get_fns_with_docstrings(src, tree)\n",
    "    #funs.update(functions)\n",
    "    # Print results\n",
    "    print(f\"Extracted functions in {i} and doc-comments:\")\n",
    "    for fn in functions:\n",
    "        print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a1930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 250\n",
      "0/250\n",
      "2/250\n",
      "4/250\n",
      "6/250\n",
      "8/250\n",
      "\n",
      "Processing chunk 0...\n",
      "✅ Done chunk 0. Got 14 new functions.\n",
      "10/250\n",
      "12/250\n",
      "14/250\n",
      "16/250\n",
      "18/250\n",
      "\n",
      "Processing chunk 1...\n",
      "✅ Done chunk 1. Got 0 new functions.\n",
      "20/250\n",
      "22/250\n",
      "24/250\n",
      "26/250\n",
      "28/250\n",
      "\n",
      "Processing chunk 2...\n",
      "✅ Done chunk 2. Got 1 new functions.\n",
      "30/250\n",
      "32/250\n",
      "34/250\n",
      "36/250\n",
      "38/250\n",
      "\n",
      "Processing chunk 3...\n",
      "✅ Done chunk 3. Got 0 new functions.\n",
      "40/250\n",
      "42/250\n",
      "44/250\n",
      "46/250\n",
      "48/250\n",
      "\n",
      "Processing chunk 4...\n",
      "✅ Done chunk 4. Got 4 new functions.\n",
      "50/250\n",
      "52/250\n",
      "54/250\n",
      "56/250\n",
      "58/250\n",
      "\n",
      "Processing chunk 5...\n",
      "✅ Done chunk 5. Got 1 new functions.\n",
      "60/250\n",
      "62/250\n",
      "64/250\n",
      "66/250\n",
      "68/250\n",
      "\n",
      "Processing chunk 6...\n",
      "✅ Done chunk 6. Got 0 new functions.\n",
      "70/250\n",
      "72/250\n",
      "74/250\n",
      "76/250\n",
      "78/250\n",
      "\n",
      "Processing chunk 7...\n",
      "✅ Done chunk 7. Got 10 new functions.\n",
      "80/250\n",
      "82/250\n",
      "84/250\n",
      "86/250\n",
      "88/250\n",
      "\n",
      "Processing chunk 8...\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "def process_chunk(idx_and_chunk):\n",
    "    assert PARSERS is not None\n",
    "    idx, chunk = idx_and_chunk\n",
    "    parser = PARSERS[idx]\n",
    "    chunk_new_funs = set()\n",
    "    \n",
    "    for ex in chunk:\n",
    "        functions = parse_ex(parser, ex)\n",
    "        for fn in functions:\n",
    "            chunk_new_funs.add(str(fn))  # <=== Fix here\n",
    "\n",
    "    return chunk_new_funs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUMWORKERS = 1\n",
    "CHUNK_SIZE = 10  # Adjust if needed\n",
    "\n",
    "PARSERS = [make_parser() for _ in range(NUMWORKERS)]\n",
    "\n",
    "funs = set()\n",
    "chunk = []\n",
    "\n",
    "total_len = len(ds)\n",
    "\n",
    "print(f\"Total dataset size: {total_len}\")\n",
    "\n",
    "# Loop over dataset\n",
    "for i, ex in enumerate(iter(ds)):\n",
    "    if i % (max(total_len // 100, 1)) == 0:\n",
    "        print(f\"{i}/{total_len}\")\n",
    "\n",
    "    chunk.append(ex)\n",
    "\n",
    "    if len(chunk) >= CHUNK_SIZE or i == total_len - 1:\n",
    "        print(f\"\\nProcessing chunk {i // CHUNK_SIZE}...\")\n",
    "\n",
    "        # Split the chunk into subchunks\n",
    "        subchunk_size = max(1, len(chunk) // NUMWORKERS)\n",
    "        subchunks = [chunk[j:j + subchunk_size] for j in range(0, len(chunk), subchunk_size)]\n",
    "\n",
    "        len_before = len(funs)\n",
    "\n",
    "        # Sequentially process each subchunk using process_chunk\n",
    "        for idx, subchunk in enumerate(subchunks):\n",
    "            chunk_funs = process_chunk((idx, subchunk))\n",
    "            funs.update(chunk_funs)\n",
    "\n",
    "        print(f\"✅ Done chunk {i // CHUNK_SIZE}. Got {len(funs) - len_before} new functions.\")\n",
    "\n",
    "        chunk = []  # Reset chunk\n",
    "        PARSERS = [make_parser() for _ in range(NUMWORKERS)]  # Rebuild parsers if needed\n",
    "\n",
    "# Final dataset creation\n",
    "print(f\"\\nTotal unique functions collected: {len(funs)}\")\n",
    "\n",
    "new_ds_dict = {\n",
    "    \"content\": list(funs),\n",
    "    \"id\": list(range(len(funs)))\n",
    "}\n",
    "\n",
    "new_ds = datasets.Dataset.from_dict(new_ds_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce4e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 48/48 [00:00<00:00, 6663.58 examples/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': \"{'function_name': 'swith_off', 'docstring': '// Turnoff Relay 1'}\",\n",
       " 'id': 7}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ds.save_to_disk(\"./extracted_functions_cpp\")\n",
    "new_ds[7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
