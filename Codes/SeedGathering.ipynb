{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805e9d4c-f390-4aba-9ec8-627b9ed85aea",
   "metadata": {},
   "source": [
    "### SEED GATHERING GET CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a752eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++++++++++++++ Run this first time if you haven't installed from requirements.txt file/cloned the repo+++++++++++++++++++++\n",
    "# !pip install tree-sitter==0.20.4\n",
    "# !git clone https://github.com/tree-sitter/tree-sitter-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd3e144-73cb-42fa-9550-075c51686a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter_parser import LANGUAGE, make_parser, node_to_string\n",
    "import datasets\n",
    "import os\n",
    "import signal\n",
    "from multiprocessing import Pool\n",
    "#import os\n",
    "import boto3\n",
    "import smart_open\n",
    "#from datasets import load_dataset,Dataset\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "def download_contents(blob_id, src_encoding):\n",
    "    s3_url = f\"s3://softwareheritage/content/{blob_id}\"\n",
    "    with smart_open.open(s3_url, \"rb\", compression=\".gz\", transport_params={\"client\": s3}) as fin:\n",
    "        content = fin.read().decode(src_encoding)\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c8ee925-5001-4a30-b29f-5741f8d173a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPLEVEL_DOC_COMMENT_QUERY = LANGUAGE.query(\"\"\"\n",
    "(\n",
    "  (function_definition\n",
    "    declarator: (function_declarator\n",
    "      declarator: (identifier) @fn-name\n",
    "    )\n",
    "    body: (compound_statement\n",
    "      (comment) @doc.comment\n",
    "    )\n",
    "  ) @function.def\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "'''\n",
    "def get_fns_with_docstrings(src, tree):\n",
    "    captures = TOPLEVEL_DOC_COMMENT_QUERY.captures(tree.root_node)\n",
    "    res = []\n",
    "    for capture in captures:\n",
    "        node, ty = capture\n",
    "        if ty != \"function.def\":\n",
    "            continue\n",
    "        # if the starting col is not 0, then it's not a top-level fn\n",
    "        _, col = node.start_point\n",
    "        if col != 0:\n",
    "            continue\n",
    "        res.append(node_to_string(src, node))\n",
    "    return res\n",
    "'''\n",
    "\n",
    "def get_fns_with_docstrings(src, tree):\n",
    "    captures = TOPLEVEL_DOC_COMMENT_QUERY.captures(tree.root_node)\n",
    "    res = []\n",
    "    current = {\"function_node\": None, \"name\": None, \"doc\": None}\n",
    "\n",
    "    for node, capture_name in captures:\n",
    "        if capture_name == \"fn-name\":\n",
    "            current[\"name\"] = node_to_string(src, node)\n",
    "\n",
    "        elif capture_name == \"doc.comment\":\n",
    "            current[\"doc\"] = node_to_string(src, node)\n",
    "\n",
    "        elif capture_name == \"function.def\":\n",
    "            current[\"function_node\"] = node\n",
    "\n",
    "            # Build the result once we have everything\n",
    "            if current[\"name\"] and current[\"doc\"]:\n",
    "                full_func_text = node_to_string(src, current[\"function_node\"])\n",
    "                res.append({\n",
    "                    \"function_name\": current[\"name\"],\n",
    "                    \"docstring\": current[\"doc\"],\n",
    "                    \"code\": full_func_text\n",
    "                })\n",
    "\n",
    "            # Reset for next\n",
    "            current = {\"function_node\": None, \"name\": None, \"doc\": None}\n",
    "\n",
    "    return res\n",
    "\n",
    "def parse_ex(parser, ex):\n",
    "    #ex = ex[\"content\"]\n",
    "    ex = download_contents(ex[\"blob_id\"], ex[\"src_encoding\"])\n",
    "    try:\n",
    "        buf = bytes(ex, \"utf8\")\n",
    "        tree = parser.parse(buf)\n",
    "        return get_fns_with_docstrings(buf, tree)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "# if one parser segfaults, we can just make a new one and other parsers will still be fine\n",
    "# WE LOVE TREE SITTER!\n",
    "PARSERS = None\n",
    "\n",
    "\n",
    "def process_chunk(idx_and_chunk):\n",
    "    assert PARSERS is not None\n",
    "    idx, chunk = idx_and_chunk\n",
    "    parser = PARSERS[idx]\n",
    "    chunk_new_funs = set()\n",
    "    for ex in chunk:\n",
    "        chunk_new_funs.update(parse_ex(parser, ex))\n",
    "    return chunk_new_funs\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    global PARSERS\n",
    "    ds = datasets.load_dataset(\n",
    "        args.dataset,\n",
    "        data_dir=args.data_dir,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    funs = set()\n",
    "    PARSERS = [make_parser() for _ in range(args.num_workers)]\n",
    "    total_len = len(ds)\n",
    "    CHUNK_SIZE = 1000 * args.num_workers\n",
    "\n",
    "    print(f\"Total length: {total_len}\")\n",
    "    print(f\"Chunk size: {CHUNK_SIZE}\")\n",
    "\n",
    "    chunk = []\n",
    "    p = Pool(args.num_workers)\n",
    "    for i, ex in enumerate(ds):\n",
    "        if i % (total_len // 100) == 0:\n",
    "            print(f\"{i}/{total_len}\")\n",
    "        try:\n",
    "            chunk.append(ex)\n",
    "            if len(chunk) == CHUNK_SIZE or i == total_len - 1:\n",
    "                print(f\"Processing chunk {i // CHUNK_SIZE}\")\n",
    "                # divide the chunk into NUM_WORKERS chunks\n",
    "                subchunk_size = len(chunk) // args.num_workers\n",
    "                subchunks = [chunk[i:i + subchunk_size]\n",
    "                             for i in range(0, len(chunk), subchunk_size)]\n",
    "                new_funs_iter = p.imap(\n",
    "                    process_chunk, [(i, subchunk) for i, subchunk in enumerate(subchunks)])\n",
    "                print(\"Getting new functions\")\n",
    "                len_before = len(funs)\n",
    "                while True:\n",
    "                    try:\n",
    "                        def timeout_handler(_, __):\n",
    "                            raise KeyboardInterrupt  # it's fineeeeeee\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(60)\n",
    "                        funs.update(next(new_funs_iter))\n",
    "                        signal.alarm(0)\n",
    "                    except KeyboardInterrupt:\n",
    "                        signal.alarm(0)\n",
    "                        print(\"Keyboard interrupt. Terminating pool\")\n",
    "                        p.terminate()\n",
    "                        p = Pool(args.num_workers)\n",
    "                        break\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                signal.alarm(0)\n",
    "\n",
    "                PARSERS = [make_parser() for _ in range(args.num_workers)]\n",
    "\n",
    "                print(\n",
    "                    f\"Done processing chunk {i // CHUNK_SIZE}. Got {len(funs) - len_before} new functions\")\n",
    "\n",
    "                chunk = []\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            chunk = []\n",
    "\n",
    "        if i == total_len - 1:\n",
    "            break\n",
    "\n",
    "    p.close()\n",
    "\n",
    "    new_ds_dict = {\n",
    "        \"content\": list(funs),\n",
    "        \"id\": list(range(len(funs)))\n",
    "    }\n",
    "\n",
    "    new_ds = datasets.Dataset.from_dict(new_ds_dict)\n",
    "    #new_ds.push_to_hub(args.push, private=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74accea3-de2a-4b38-bbf2-b0c7f2be7aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMWORKERS = os.cpu_count()\n",
    "NUMWORKERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8798ed1-24c7-4694-a97a-a381ab122392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1332eec6916b44568331e600521ffde5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"bigcode/the-stack-v2-dedup\", \"C++\", cache_dir=f\"./cache/stack\", streaming=False, split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6095d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import islice\n",
    "\n",
    "# small_subset = islice(ds, 50)\n",
    "\n",
    "# # Convert to list if you want to materialize it (use with caution, as this loads into memory)\n",
    "# ds = list(small_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b23a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted functions in 0 and doc-comments:\n",
      "Extracted functions in 1 and doc-comments:\n",
      "Extracted functions in 2 and doc-comments:\n",
      "Extracted functions in 3 and doc-comments:\n",
      "Extracted functions in 4 and doc-comments:\n"
     ]
    }
   ],
   "source": [
    "# Setup a single Parser\n",
    "funs = set()\n",
    "\n",
    "parser = make_parser()\n",
    "\n",
    "for i in range(5):\n",
    "# Take one example manually\n",
    "    ex = ds[i]  # First example (directly)\n",
    "\n",
    "    # Download content if needed\n",
    "    content = download_contents(ex[\"blob_id\"], ex[\"src_encoding\"])\n",
    "\n",
    "    # Parse\n",
    "    src = bytes(content, \"utf8\")\n",
    "    tree = parser.parse(src)\n",
    "\n",
    "    # Extract functions\n",
    "    functions = get_fns_with_docstrings(src, tree)\n",
    "    #funs.update(functions)\n",
    "    # Print results\n",
    "    print(f\"Extracted functions in {i} and doc-comments:\")\n",
    "    for fn in functions:\n",
    "        print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9a1930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 1000\n",
      "0/1000\n",
      "\n",
      "Processing chunk 0...\n",
      "✅ Done chunk 0. Got 14 new functions.\n",
      "10/1000\n",
      "\n",
      "Processing chunk 1...\n",
      "✅ Done chunk 1. Got 0 new functions.\n",
      "20/1000\n",
      "\n",
      "Processing chunk 2...\n",
      "✅ Done chunk 2. Got 1 new functions.\n",
      "30/1000\n",
      "\n",
      "Processing chunk 3...\n",
      "✅ Done chunk 3. Got 0 new functions.\n",
      "40/1000\n",
      "\n",
      "Processing chunk 4...\n",
      "✅ Done chunk 4. Got 4 new functions.\n",
      "50/1000\n",
      "\n",
      "Processing chunk 5...\n",
      "✅ Done chunk 5. Got 1 new functions.\n",
      "60/1000\n",
      "\n",
      "Processing chunk 6...\n",
      "✅ Done chunk 6. Got 0 new functions.\n",
      "70/1000\n",
      "\n",
      "Processing chunk 7...\n",
      "✅ Done chunk 7. Got 10 new functions.\n",
      "80/1000\n",
      "\n",
      "Processing chunk 8...\n",
      "✅ Done chunk 8. Got 3 new functions.\n",
      "90/1000\n",
      "\n",
      "Processing chunk 9...\n",
      "✅ Done chunk 9. Got 0 new functions.\n",
      "100/1000\n",
      "\n",
      "Processing chunk 10...\n",
      "✅ Done chunk 10. Got 0 new functions.\n",
      "110/1000\n",
      "\n",
      "Processing chunk 11...\n",
      "✅ Done chunk 11. Got 0 new functions.\n",
      "120/1000\n",
      "\n",
      "Processing chunk 12...\n",
      "✅ Done chunk 12. Got 0 new functions.\n",
      "130/1000\n",
      "\n",
      "Processing chunk 13...\n",
      "✅ Done chunk 13. Got 0 new functions.\n",
      "140/1000\n",
      "\n",
      "Processing chunk 14...\n",
      "✅ Done chunk 14. Got 0 new functions.\n",
      "150/1000\n",
      "\n",
      "Processing chunk 15...\n",
      "✅ Done chunk 15. Got 2 new functions.\n",
      "160/1000\n",
      "\n",
      "Processing chunk 16...\n",
      "✅ Done chunk 16. Got 5 new functions.\n",
      "170/1000\n",
      "\n",
      "Processing chunk 17...\n",
      "✅ Done chunk 17. Got 1 new functions.\n",
      "180/1000\n",
      "\n",
      "Processing chunk 18...\n",
      "✅ Done chunk 18. Got 4 new functions.\n",
      "190/1000\n",
      "\n",
      "Processing chunk 19...\n",
      "✅ Done chunk 19. Got 0 new functions.\n",
      "200/1000\n",
      "\n",
      "Processing chunk 20...\n",
      "✅ Done chunk 20. Got 0 new functions.\n",
      "210/1000\n",
      "\n",
      "Processing chunk 21...\n",
      "✅ Done chunk 21. Got 0 new functions.\n",
      "220/1000\n",
      "\n",
      "Processing chunk 22...\n",
      "✅ Done chunk 22. Got 2 new functions.\n",
      "230/1000\n",
      "\n",
      "Processing chunk 23...\n",
      "✅ Done chunk 23. Got 1 new functions.\n",
      "240/1000\n",
      "\n",
      "Processing chunk 24...\n",
      "✅ Done chunk 24. Got 0 new functions.\n",
      "250/1000\n",
      "\n",
      "Processing chunk 25...\n",
      "✅ Done chunk 25. Got 4 new functions.\n",
      "260/1000\n",
      "\n",
      "Processing chunk 26...\n",
      "✅ Done chunk 26. Got 2 new functions.\n",
      "270/1000\n",
      "\n",
      "Processing chunk 27...\n",
      "✅ Done chunk 27. Got 22 new functions.\n",
      "280/1000\n",
      "\n",
      "Processing chunk 28...\n",
      "✅ Done chunk 28. Got 1 new functions.\n",
      "290/1000\n",
      "\n",
      "Processing chunk 29...\n",
      "✅ Done chunk 29. Got 0 new functions.\n",
      "300/1000\n",
      "\n",
      "Processing chunk 30...\n",
      "✅ Done chunk 30. Got 0 new functions.\n",
      "310/1000\n",
      "\n",
      "Processing chunk 31...\n",
      "✅ Done chunk 31. Got 0 new functions.\n",
      "320/1000\n",
      "\n",
      "Processing chunk 32...\n",
      "✅ Done chunk 32. Got 1 new functions.\n",
      "330/1000\n",
      "\n",
      "Processing chunk 33...\n",
      "✅ Done chunk 33. Got 0 new functions.\n",
      "340/1000\n",
      "\n",
      "Processing chunk 34...\n",
      "✅ Done chunk 34. Got 0 new functions.\n",
      "350/1000\n",
      "\n",
      "Processing chunk 35...\n",
      "✅ Done chunk 35. Got 0 new functions.\n",
      "360/1000\n",
      "\n",
      "Processing chunk 36...\n",
      "✅ Done chunk 36. Got 1 new functions.\n",
      "370/1000\n",
      "\n",
      "Processing chunk 37...\n",
      "✅ Done chunk 37. Got 5 new functions.\n",
      "380/1000\n",
      "\n",
      "Processing chunk 38...\n",
      "✅ Done chunk 38. Got 6 new functions.\n",
      "390/1000\n",
      "\n",
      "Processing chunk 39...\n",
      "✅ Done chunk 39. Got 14 new functions.\n",
      "400/1000\n",
      "\n",
      "Processing chunk 40...\n",
      "✅ Done chunk 40. Got 0 new functions.\n",
      "410/1000\n",
      "\n",
      "Processing chunk 41...\n",
      "✅ Done chunk 41. Got 1 new functions.\n",
      "420/1000\n",
      "\n",
      "Processing chunk 42...\n",
      "✅ Done chunk 42. Got 1 new functions.\n",
      "430/1000\n",
      "\n",
      "Processing chunk 43...\n",
      "✅ Done chunk 43. Got 1 new functions.\n",
      "440/1000\n",
      "\n",
      "Processing chunk 44...\n",
      "✅ Done chunk 44. Got 0 new functions.\n",
      "450/1000\n",
      "\n",
      "Processing chunk 45...\n",
      "✅ Done chunk 45. Got 0 new functions.\n",
      "460/1000\n",
      "\n",
      "Processing chunk 46...\n",
      "✅ Done chunk 46. Got 4 new functions.\n",
      "470/1000\n",
      "\n",
      "Processing chunk 47...\n",
      "✅ Done chunk 47. Got 0 new functions.\n",
      "480/1000\n",
      "\n",
      "Processing chunk 48...\n",
      "✅ Done chunk 48. Got 11 new functions.\n",
      "490/1000\n",
      "\n",
      "Processing chunk 49...\n",
      "✅ Done chunk 49. Got 0 new functions.\n",
      "500/1000\n",
      "\n",
      "Processing chunk 50...\n",
      "✅ Done chunk 50. Got 1 new functions.\n",
      "510/1000\n",
      "\n",
      "Processing chunk 51...\n",
      "✅ Done chunk 51. Got 0 new functions.\n",
      "520/1000\n",
      "\n",
      "Processing chunk 52...\n",
      "✅ Done chunk 52. Got 3 new functions.\n",
      "530/1000\n",
      "\n",
      "Processing chunk 53...\n",
      "✅ Done chunk 53. Got 5 new functions.\n",
      "540/1000\n",
      "\n",
      "Processing chunk 54...\n",
      "✅ Done chunk 54. Got 1 new functions.\n",
      "550/1000\n",
      "\n",
      "Processing chunk 55...\n",
      "✅ Done chunk 55. Got 0 new functions.\n",
      "560/1000\n",
      "\n",
      "Processing chunk 56...\n",
      "✅ Done chunk 56. Got 0 new functions.\n",
      "570/1000\n",
      "\n",
      "Processing chunk 57...\n",
      "✅ Done chunk 57. Got 0 new functions.\n",
      "580/1000\n",
      "\n",
      "Processing chunk 58...\n",
      "✅ Done chunk 58. Got 0 new functions.\n",
      "590/1000\n",
      "\n",
      "Processing chunk 59...\n",
      "✅ Done chunk 59. Got 0 new functions.\n",
      "600/1000\n",
      "\n",
      "Processing chunk 60...\n",
      "✅ Done chunk 60. Got 2 new functions.\n",
      "610/1000\n",
      "\n",
      "Processing chunk 61...\n",
      "✅ Done chunk 61. Got 3 new functions.\n",
      "620/1000\n",
      "\n",
      "Processing chunk 62...\n",
      "✅ Done chunk 62. Got 0 new functions.\n",
      "630/1000\n",
      "\n",
      "Processing chunk 63...\n",
      "✅ Done chunk 63. Got 1 new functions.\n",
      "640/1000\n",
      "\n",
      "Processing chunk 64...\n",
      "✅ Done chunk 64. Got 0 new functions.\n",
      "650/1000\n",
      "\n",
      "Processing chunk 65...\n",
      "✅ Done chunk 65. Got 3 new functions.\n",
      "660/1000\n",
      "\n",
      "Processing chunk 66...\n",
      "✅ Done chunk 66. Got 1 new functions.\n",
      "670/1000\n",
      "\n",
      "Processing chunk 67...\n",
      "✅ Done chunk 67. Got 2 new functions.\n",
      "680/1000\n",
      "\n",
      "Processing chunk 68...\n",
      "✅ Done chunk 68. Got 1 new functions.\n",
      "690/1000\n",
      "\n",
      "Processing chunk 69...\n",
      "✅ Done chunk 69. Got 6 new functions.\n",
      "700/1000\n",
      "\n",
      "Processing chunk 70...\n",
      "✅ Done chunk 70. Got 0 new functions.\n",
      "710/1000\n",
      "\n",
      "Processing chunk 71...\n",
      "✅ Done chunk 71. Got 1 new functions.\n",
      "720/1000\n",
      "\n",
      "Processing chunk 72...\n",
      "✅ Done chunk 72. Got 0 new functions.\n",
      "730/1000\n",
      "\n",
      "Processing chunk 73...\n",
      "✅ Done chunk 73. Got 0 new functions.\n",
      "740/1000\n",
      "\n",
      "Processing chunk 74...\n",
      "✅ Done chunk 74. Got 0 new functions.\n",
      "750/1000\n",
      "\n",
      "Processing chunk 75...\n",
      "✅ Done chunk 75. Got 0 new functions.\n",
      "760/1000\n",
      "\n",
      "Processing chunk 76...\n",
      "✅ Done chunk 76. Got 1 new functions.\n",
      "770/1000\n",
      "\n",
      "Processing chunk 77...\n",
      "✅ Done chunk 77. Got 5 new functions.\n",
      "780/1000\n",
      "\n",
      "Processing chunk 78...\n",
      "✅ Done chunk 78. Got 4 new functions.\n",
      "790/1000\n",
      "\n",
      "Processing chunk 79...\n",
      "✅ Done chunk 79. Got 0 new functions.\n",
      "800/1000\n",
      "\n",
      "Processing chunk 80...\n",
      "✅ Done chunk 80. Got 0 new functions.\n",
      "810/1000\n",
      "\n",
      "Processing chunk 81...\n",
      "✅ Done chunk 81. Got 1 new functions.\n",
      "820/1000\n",
      "\n",
      "Processing chunk 82...\n",
      "✅ Done chunk 82. Got 0 new functions.\n",
      "830/1000\n",
      "\n",
      "Processing chunk 83...\n",
      "✅ Done chunk 83. Got 8 new functions.\n",
      "840/1000\n",
      "\n",
      "Processing chunk 84...\n",
      "✅ Done chunk 84. Got 2 new functions.\n",
      "850/1000\n",
      "\n",
      "Processing chunk 85...\n",
      "✅ Done chunk 85. Got 0 new functions.\n",
      "860/1000\n",
      "\n",
      "Processing chunk 86...\n",
      "✅ Done chunk 86. Got 0 new functions.\n",
      "870/1000\n",
      "\n",
      "Processing chunk 87...\n",
      "✅ Done chunk 87. Got 5 new functions.\n",
      "880/1000\n",
      "\n",
      "Processing chunk 88...\n",
      "✅ Done chunk 88. Got 1 new functions.\n",
      "890/1000\n",
      "\n",
      "Processing chunk 89...\n",
      "✅ Done chunk 89. Got 0 new functions.\n",
      "900/1000\n",
      "\n",
      "Processing chunk 90...\n",
      "✅ Done chunk 90. Got 0 new functions.\n",
      "910/1000\n",
      "\n",
      "Processing chunk 91...\n",
      "✅ Done chunk 91. Got 0 new functions.\n",
      "920/1000\n",
      "\n",
      "Processing chunk 92...\n",
      "✅ Done chunk 92. Got 0 new functions.\n",
      "930/1000\n",
      "\n",
      "Processing chunk 93...\n",
      "✅ Done chunk 93. Got 6 new functions.\n",
      "940/1000\n",
      "\n",
      "Processing chunk 94...\n",
      "✅ Done chunk 94. Got 14 new functions.\n",
      "950/1000\n",
      "\n",
      "Processing chunk 95...\n",
      "✅ Done chunk 95. Got 0 new functions.\n",
      "960/1000\n",
      "\n",
      "Processing chunk 96...\n",
      "✅ Done chunk 96. Got 4 new functions.\n",
      "970/1000\n",
      "\n",
      "Processing chunk 97...\n",
      "✅ Done chunk 97. Got 3 new functions.\n",
      "980/1000\n",
      "\n",
      "Processing chunk 98...\n",
      "✅ Done chunk 98. Got 4 new functions.\n",
      "990/1000\n",
      "\n",
      "Processing chunk 99...\n",
      "✅ Done chunk 99. Got 0 new functions.\n",
      "\n",
      "Total unique functions collected: 210\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "def process_chunk(idx_and_chunk):\n",
    "    assert PARSERS is not None\n",
    "    idx, chunk = idx_and_chunk\n",
    "    parser = PARSERS[idx]\n",
    "    chunk_new_funs = set()\n",
    "    \n",
    "    for ex in chunk:\n",
    "        functions = parse_ex(parser, ex)\n",
    "        for fn in functions:\n",
    "            chunk_new_funs.add(str(fn))  # <=== Fix here\n",
    "\n",
    "    return chunk_new_funs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUMWORKERS = 1\n",
    "CHUNK_SIZE = 10  # Adjust if needed\n",
    "\n",
    "PARSERS = [make_parser() for _ in range(NUMWORKERS)]\n",
    "\n",
    "funs = set()\n",
    "chunk = []\n",
    "\n",
    "total_len = len(ds)\n",
    "\n",
    "print(f\"Total dataset size: {total_len}\")\n",
    "\n",
    "# Loop over dataset\n",
    "for i, ex in enumerate(iter(ds)):\n",
    "    if i % (max(total_len // 100, 1)) == 0:\n",
    "        print(f\"{i}/{total_len}\")\n",
    "\n",
    "    chunk.append(ex)\n",
    "\n",
    "    if len(chunk) >= CHUNK_SIZE or i == total_len - 1:\n",
    "        print(f\"\\nProcessing chunk {i // CHUNK_SIZE}...\")\n",
    "\n",
    "        # Split the chunk into subchunks\n",
    "        subchunk_size = max(1, len(chunk) // NUMWORKERS)\n",
    "        subchunks = [chunk[j:j + subchunk_size] for j in range(0, len(chunk), subchunk_size)]\n",
    "\n",
    "        len_before = len(funs)\n",
    "\n",
    "        # Sequentially process each subchunk using process_chunk\n",
    "        for idx, subchunk in enumerate(subchunks):\n",
    "            chunk_funs = process_chunk((idx, subchunk))\n",
    "            funs.update(chunk_funs)\n",
    "\n",
    "        print(f\"✅ Done chunk {i // CHUNK_SIZE}. Got {len(funs) - len_before} new functions.\")\n",
    "\n",
    "        chunk = []  # Reset chunk\n",
    "        PARSERS = [make_parser() for _ in range(NUMWORKERS)]  # Rebuild parsers if needed\n",
    "\n",
    "# Final dataset creation\n",
    "print(f\"\\nTotal unique functions collected: {len(funs)}\")\n",
    "\n",
    "new_ds_dict = {\n",
    "    \"content\": list(funs),\n",
    "    \"id\": list(range(len(funs)))\n",
    "}\n",
    "\n",
    "new_ds = datasets.Dataset.from_dict(new_ds_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ce4e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2055e71c45e54eb6a0db02b588faa741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'content': \"{'function_name': 'Max', 'docstring': '//求n个数的max ', 'code': 'void push_down(Splay *x){//下放标记 \\\\n\\\\tif(x==null)return;\\\\n\\\\tif(x->rev){//区间翻转 \\\\n\\\\t\\\\tx->rev=0,x->son[0]->rev^=1,x->son[1]->rev^=1;\\\\n\\\\t\\\\tswap(x->son[0],x->son[1]),swap(x->maxl,x->maxr);\\\\n\\\\t}\\\\n\\\\tif(x->same){//区间赋值 \\\\n\\\\t\\\\tx->same=0,x->son[0]->key=x->son[1]->key=x->key;\\\\n\\\\t\\\\tx->son[0]->same=x->son[1]->same=1;\\\\n\\\\t\\\\tx->sum=x->key*x->size;\\\\n\\\\t\\\\tx->maxl=x->maxr=x->maxt=(x->key>0?x->sum:x->key);\\\\n\\\\t}\\\\n}'}\",\n",
       " 'id': 7}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ds.save_to_disk(\"./extracted_functions_cpp\")\n",
    "new_ds[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086c5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"./extracted_functions_cpp\")\n",
    "\n",
    "# for example in ds:\n",
    "#     print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6741a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "from tree_sitter_parser import LANGUAGE, make_parser, does_have_return\n",
    "\n",
    "# Define return-statement query for C++\n",
    "RETURN_QUERY = LANGUAGE.query(\"\"\"\n",
    "(return_statement) @return\n",
    "\"\"\")\n",
    "\n",
    "# Use a Tree-sitter parser set to C++\n",
    "parser = make_parser()\n",
    "\n",
    "# Filter dataset to only functions with meaningful return values\n",
    "def filter_cpp_functions_with_return(ds: Dataset) -> Dataset:\n",
    "    filtered_ds = []\n",
    "    for i in ds:\n",
    "        if does_have_return(i[\"content\"], parser):\n",
    "            filtered_ds.append(i)\n",
    "    return filtered_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa5a69f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering to only C++ functions with return statements...\n",
      "✅ Filtered dataset size: 73\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering to only C++ functions with return statements...\")\n",
    "filtered_ds = filter_cpp_functions_with_return(ds)\n",
    "print(f\"✅ Filtered dataset size: {len(filtered_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eec27f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f172b032e9447eb3967dd782e2323e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/73 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert the list into a Dataset\n",
    "filtered_ds = Dataset.from_list(filtered_ds)\n",
    "\n",
    "# Now you can save it to disk\n",
    "filtered_ds.save_to_disk(\"./functions_with_return_cpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e55f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to filter out functions that have valid types is next but I'm not sure we need it for c++ as functions are already typed\n",
    "#Should check with TA if it is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "133ec49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"./functions_with_return_cpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83e81424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_name': 'loop',\n",
       " 'docstring': '/*\\n    //MOSTRAR INFO EN LED\\n    if (isDoorOpen==true) {\\n      ledToggle(\"on\");\\n    } else {\\n      ledToggle(\"off\");\\n    }\\n\\n    //MOSTRAR INFO EN LED\\n    if (hasDetection==true) {\\n      ledToggle(\"on\");\\n    } else {\\n      ledToggle(\"off\");\\n    }\\n*/',\n",
       " 'code': 'int setColor(String command)    {\\n    // Look through the list of colors to find the one that was requested\\n    for(int iColor = 0; iColor < NUM_COLORS; iColor++)\\n    {\\n        if(command == colorName[iColor]) {\\n            // When it matches, look up the RGB values for that color in the table,\\n            // and write the red, green, and blue values.\\n            RGB.control(true);\\n            RGB.color(colorRGB[iColor][0], colorRGB[iColor][1], colorRGB[iColor][2]);\\n\\n            analogWrite(pinRed,colorRGB[iColor][0]);\\n            analogWrite(pinGreen,colorRGB[iColor][1]);\\n            analogWrite(pinBlue,colorRGB[iColor][2]);\\n            return 0;\\n        }\\n    }\\n\\n    return -1;\\n}'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
<<<<<<< HEAD
    "# content = ast.literal_eval(ds[0][\"content\"])\n",
    "# print(content['code'], '\\n')\n",
    "\n",
    "for i in range(73):\n",
    "    try:\n",
    "        content = ast.literal_eval(ds[i][\"content\"])\n",
    "        print(content['function_name'])\n",
    "        print(content['code'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing content: {e}\")\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2724f9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You have to get the code by ast.literal_eval(ds[i][\"content\"])['code]\n",
    "\n",
    "#######################################################################\n",
    "#                     Part 3\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71861e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "from tree_sitter_parser import global_parser, LANGUAGE, does_have_return, make_parser\n",
    "#import benchmark_data\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import argparse\n",
    "#from vllm import LLM, SamplingParams\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9628569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FN_BLOCK_QUERY = LANGUAGE.query(\"\"\"\n",
    "(function_definition\n",
    "  body: (compound_statement) @fn-block)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def template_few_shot(code, answer, rationale):\n",
    "    doc, code = cpp_extract_docstring(code)\n",
    "    assert answer == \"No\" or answer == \"Yes\"\n",
    "    prompt = f\"\"\"<issue_start>username_0: I have a function in C++ and I'd like someone to check my description of this function.\n",
    "I'm doing this so that I can write a good docstring for this function.\n",
    "\n",
    "Here is the code for the function:\n",
    "```py\n",
    "{code}\n",
    "```\n",
    "\n",
    "Here is my description of this program:\n",
    "```\n",
    "{doc}\n",
    "```\n",
    "\n",
    "Do not attempt to execute the function or to judge its correctness.\n",
    "Answer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function.\n",
    "Also, answer with \"No\" if the description does not match the function.<issue_comment>username_1: Sure, no problem. I will be able to help.\n",
    "My answer is: {answer}\n",
    "\n",
    "{rationale}\n",
    "\n",
    "Upvotes: 200\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "FEW_SHOTS = [\n",
    "    (''' \n",
    "    /**\n",
    "    * Transposes a given 2D matrix of integers.\n",
    "    * Input: A 2D vector representing the matrix.\n",
    "    * Output: A new 2D vector with rows and columns swapped.\n",
    "    */\n",
    "    std::vector<std::vector<int>> transposeMatrix(const std::vector<std::vector<int>>& matrix) {\n",
    "        if (matrix.empty()) return {};\n",
    "\n",
    "        size_t rows = matrix.size();\n",
    "        size_t cols = matrix[0].size();\n",
    "        std::vector<std::vector<int>> transposed(cols, std::vector<int>(rows));\n",
    "\n",
    "        for (size_t i = 0; i < rows; ++i)\n",
    "            for (size_t j = 0; j < cols; ++j)\n",
    "                transposed[j][i] = matrix[i][j];\n",
    "\n",
    "        return transposed;\n",
    "    }''',\n",
    "    \"Yes\",\n",
    "    \"The docstring clearly describes the input, output, and behavior of the function without ambiguity.\"\n",
    "    ),\n",
    "    ('''\n",
    "    /**\n",
    "    * Helper function to open input and output file streams.\n",
    "    */\n",
    "    void initFileStreams(const std::string& inputFile, const std::string& outputFile,\n",
    "                        std::ifstream& inStream, std::ofstream& outStream) {\n",
    "        inStream.open(inputFile);\n",
    "        outStream.open(outputFile);\n",
    "        if (!inStream.is_open() || !outStream.is_open()) {\n",
    "            std::cerr << \"Failed to open files.\\n\";\n",
    "        }\n",
    "    }''',\n",
    "    \"No\",\n",
    "    \"The docstring only says it's a helper without describing what files are opened, how, or error behavior. It’s vague.\"\n",
    "    ),\n",
    "    (''' \n",
    "    /**\n",
    "    * Reads a text file and counts the frequency of each word (case-insensitive).\n",
    "    * Returns a map where keys are words and values are frequency counts.\n",
    "    */\n",
    "    std::unordered_map<std::string, int> countWordFrequencies(const std::string& filePath) {\n",
    "        std::unordered_map<std::string, int> wordCount;\n",
    "        std::ifstream file(filePath);\n",
    "        std::string word;\n",
    "\n",
    "        if (!file.is_open()) {\n",
    "            std::cerr << \"Error opening file.\\n\";\n",
    "            return wordCount;\n",
    "        }\n",
    "\n",
    "        while (file >> word) {\n",
    "            // Convert to lowercase\n",
    "            for (char& c : word) c = std::tolower(c);\n",
    "            // Remove punctuation\n",
    "            word.erase(std::remove_if(word.begin(), word.end(),\n",
    "                                    [](char ch) { return std::ispunct(ch); }),\n",
    "                    word.end());\n",
    "            if (!word.empty())\n",
    "                ++wordCount[word];\n",
    "        }\n",
    "\n",
    "        return wordCount;\n",
    "    }''',\n",
    "    \"Yes\",\n",
    "    \"The docstring clearly describes the purpose, behavior, and output of the function, including key aspects like case-insensitivity.\"\n",
    "        ),\n",
    "    ('''\n",
    "     /**\n",
    "    * Checks whether the given string is a palindrome.\n",
    "    * Ignores case and non-alphanumeric characters.\n",
    "    */\n",
    "    bool isPalindrome(const std::string& s) {\n",
    "        int left = 0;\n",
    "        int right = s.length() - 1;\n",
    "\n",
    "        while (left < right) {\n",
    "            while (left < right && !std::isalnum(s[left])) ++left;\n",
    "            while (left < right && !std::isalnum(s[right])) --right;\n",
    "\n",
    "            if (std::tolower(s[left]) != std::tolower(s[right]))\n",
    "                return false;\n",
    "\n",
    "            ++left;\n",
    "            --right;\n",
    "        }\n",
    "\n",
    "        return true;\n",
    "    }''',\n",
    "    \"Yes\",\n",
    "    \"The docstring is concise but accurately specifies the functionality and relevant behavior like ignoring non-alphanumerics and case.\"\n",
    "    ),\n",
    "    ('''\n",
    "     /**\n",
    "    * Resizes a raw RGB image buffer by scaling it to a target width and height.\n",
    "    */\n",
    "    unsigned char* resizeImage(const unsigned char* inputBuffer, int width, int height,\n",
    "                            int newWidth, int newHeight) {\n",
    "        if (!inputBuffer || width <= 0 || height <= 0 || newWidth <= 0 || newHeight <= 0)\n",
    "            return nullptr;\n",
    "\n",
    "        unsigned char* output = new unsigned char[newWidth * newHeight * 3];\n",
    "\n",
    "        for (int y = 0; y < newHeight; ++y) {\n",
    "            for (int x = 0; x < newWidth; ++x) {\n",
    "                int srcX = x * width / newWidth;\n",
    "                int srcY = y * height / newHeight;\n",
    "                for (int c = 0; c < 3; ++c) {\n",
    "                    output[(y * newWidth + x) * 3 + c] =\n",
    "                        inputBuffer[(srcY * width + srcX) * 3 + c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return output;\n",
    "    }\n",
    "    ''',\n",
    "    \"No\",\n",
    "    \"The docstring mentions resizing but omits that this is a naive nearest-neighbor implementation with no memory management or edge handling info.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "import re\n",
    "\n",
    "# def cpp_extract_docstring(code):\n",
    "#     \"\"\"\n",
    "#     Extracts the first C++-style docstring (/** ... */) from the given code string.\n",
    "#     Returns a tuple of (docstring_content, remaining_code).\n",
    "#     \"\"\"\n",
    "#     # Look for /** ... */ using regex\n",
    "#     match = re.search(r'/\\*\\*(.*?)\\*/', code, re.DOTALL)\n",
    "#     assert match, \"No C++-style docstring found\"\n",
    "\n",
    "#     doc = match.group(1)\n",
    "\n",
    "#     # Clean up leading * characters and whitespace\n",
    "#     lines = doc.strip().split('\\n')\n",
    "#     cleaned_lines = []\n",
    "#     for line in lines:\n",
    "#         line = line.strip()\n",
    "#         if line.startswith(\"*\"):\n",
    "#             line = line[1:].lstrip()\n",
    "#         cleaned_lines.append(line)\n",
    "\n",
    "#     cleaned_doc = \"\\n\".join(cleaned_lines).strip()\n",
    "#     #remaining_code = code[:match.start()] + code[match.end():]\n",
    "\n",
    "#     return cleaned_doc\n",
    "\n",
    "def cpp_extract_docstring(code):\n",
    "    \"\"\"\n",
    "    Extracts the first C++-style docstring (/** ... */) from the given code string.\n",
    "    Returns a tuple of (docstring_content, remaining_code).\n",
    "    \"\"\"\n",
    "    match = re.search(r'/\\*\\*(.*?)\\*/', code, re.DOTALL)\n",
    "    if not match:\n",
    "        #logging.warning(\"No C++-style docstring found.\")\n",
    "        return \"No Docstring found\", code  # Return empty docstring and original code\n",
    "\n",
    "    doc = match.group(1)\n",
    "\n",
    "    # Clean up leading * characters and whitespace\n",
    "    lines = doc.strip().split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"*\"):\n",
    "            line = line[1:].lstrip()\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    cleaned_doc = \"\\n\".join(cleaned_lines).strip()\n",
    "    remaining_code = code[:match.start()] + code[match.end():]\n",
    "\n",
    "    return cleaned_doc, remaining_code\n",
    "\n",
    "\n",
    "def prompt_fmt(code):\n",
    "    doc, code = cpp_extract_docstring(code)\n",
    "    random.shuffle(FEW_SHOTS)\n",
    "    buf = \"\"\n",
    "    for few in FEW_SHOTS:\n",
    "        buf += template_few_shot(*few)\n",
    "    buf += f\"\"\"<issue_start>username_0: I have a function in C++ and I'd like someone to check my description of this function.\n",
    "I'm doing this so that I can write a good docstring for this function.\n",
    "\n",
    "Here is the code for the function:\n",
    "```c++\n",
    "{code}\n",
    "```\n",
    "\n",
    "Here is my description of this program:\n",
    "```\n",
    "{doc}\n",
    "```\n",
    "\n",
    "Do not attempt to execute the function or to judge its correctness.\n",
    "Answer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function.\n",
    "Also, answer with \"No\" if the description does not match the function.\n",
    "Upvotes: 100<issue_comment>username_1: Sure, no problem. I will be able to help.\n",
    "My answer is:\"\"\"\n",
    "    return buf\n",
    "\n",
    "\n",
    "def auto_dtype():\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        return \"bfloat16\"\n",
    "    return \"auto\"\n",
    "\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    chunks = []\n",
    "    for i in range(0, len(lst), n):\n",
    "        chunk = []\n",
    "        for j in range(n):\n",
    "            if i + j < len(lst):\n",
    "                chunk.append(lst[i + j])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a429c00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 examples. Running pre-filtering...\n"
     ]
    }
   ],
   "source": [
    "dataset = ds\n",
    "\n",
    "print(f\"Loaded {len(dataset)} examples. Running pre-filtering...\")\n",
    "\n",
    "BAD_COMMENT_KEYWORDS = [\"todo\", \"fixme\", \"bug\", \"hack\"]\n",
    "BAD_SUBSTRINGS = [f\"// {kw}\" for kw in BAD_COMMENT_KEYWORDS]\n",
    "\n",
    "def cpp_pre_filter(entry):\n",
    "    try:\n",
    "        content = entry[\"content\"]\n",
    "        content_dict = ast.literal_eval(content)\n",
    "        code = content_dict.get(\"code\", \"\").strip().lower()\n",
    "\n",
    "        # 1. Filter out undesirable comments like TODOs or FIXMEs\n",
    "        BAD_COMMENT_KEYWORDS = [\"todo\", \"fixme\", \"bug\", \"hack\"]\n",
    "        if any(f\"// {kw}\" in code for kw in BAD_COMMENT_KEYWORDS):\n",
    "            return False\n",
    "\n",
    "        # 2. Skip functions that are too long\n",
    "        if len(code.splitlines()) > 200:\n",
    "            return False\n",
    "\n",
    "        # 3. Skip overly short functions\n",
    "        if len(code) < 5:\n",
    "            return False\n",
    "\n",
    "        # 4. Skip known problematic includes or system calls\n",
    "        if any(x in code for x in [\"#include <windows.h>\", \"system(\"]):\n",
    "            return False\n",
    "\n",
    "        # 5. Skip if signature shows no arguments (like void fn())\n",
    "        for line in code.splitlines():\n",
    "            if \"(\" in line and \")\" in line and \"()\" in line.split(\")\")[0]:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Filter error: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1e67159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988b76b3e76244fe9c136826ce1ee892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ast \n",
    "\n",
    "threads = 1 #os.cpu_count() - 1  # type: ignore\n",
    "dataset = ds\n",
    "dataset = dataset.filter(cpp_pre_filter, num_proc=threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16df271f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'id'],\n",
       "    num_rows: 7\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5764f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LLM(f\"../../../StarCoder\", dtype=auto_dtype(), gpu_memory_utilization=0.95, tensor_parallel_size=1)\n",
    "#++++++++++++++++++TA has used above model but vLLM is an ass in Windows so we are going for alternatives\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "#model_name = \"bigcode/starcoder\"       #Bigger and better model. not to run on local system\n",
    "\n",
    "model_name = \"bigcode/starcoderbase-1b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 🔧 Fix for padding\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e0db75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot prompt has 1979 tokens\n"
     ]
    }
   ],
   "source": [
    "dummy = '/*Dummy Docstring*/\\n int dummy(){ \\n    /*\\n    */\\n return -1;}'\n",
    "dummy_prompt = prompt_fmt(dummy)\n",
    "few_shot_toks = len(tokenizer.encode(\n",
    "    dummy_prompt)) - len(tokenizer.encode(dummy))\n",
    "print(f\"Few-shot prompt has {few_shot_toks} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c25883c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<issue_start>username_0: I have a function in C++ and I\\'d like someone to check my description of this function.\\nI\\'m doing this so that I can write a good docstring for this function.\\n\\nHere is the code for the function:\\n```py\\n\\n    \\n    void initFileStreams(const std::string& inputFile, const std::string& outputFile,\\n                        std::ifstream& inStream, std::ofstream& outStream) {\\n        inStream.open(inputFile);\\n        outStream.open(outputFile);\\n        if (!inStream.is_open() || !outStream.is_open()) {\\n            std::cerr << \"Failed to open files.\\n\";\\n        }\\n    }\\n```\\n\\nHere is my description of this program:\\n```\\nHelper function to open input and output file streams.\\n```\\n\\nDo not attempt to execute the function or to judge its correctness.\\nAnswer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function.\\nAlso, answer with \"No\" if the description does not match the function.<issue_comment>username_1: Sure, no problem. I will be able to help.\\nMy answer is: No\\n\\nThe docstring only says it\\'s a helper without describing what files are opened, how, or error behavior. It’s vague.\\n\\nUpvotes: 200<issue_start>username_0: I have a function in C++ and I\\'d like someone to check my description of this function.\\nI\\'m doing this so that I can write a good docstring for this function.\\n\\nHere is the code for the function:\\n```py\\n \\n    \\n    std::unordered_map<std::string, int> countWordFrequencies(const std::string& filePath) {\\n        std::unordered_map<std::string, int> wordCount;\\n        std::ifstream file(filePath);\\n        std::string word;\\n\\n        if (!file.is_open()) {\\n            std::cerr << \"Error opening file.\\n\";\\n            return wordCount;\\n        }\\n\\n        while (file >> word) {\\n            // Convert to lowercase\\n            for (char& c : word) c = std::tolower(c);\\n            // Remove punctuation\\n            word.erase(std::remove_if(word.begin(), word.end(),\\n                                    [](char ch) { return std::ispunct(ch); }),\\n                    word.end());\\n            if (!word.empty())\\n                ++wordCount[word];\\n        }\\n\\n        return wordCount;\\n    }\\n```\\n\\nHere is my description of this program:\\n```\\nReads a text file and counts the frequency of each word (case-insensitive).\\nReturns a map where keys are words and values are frequency counts.\\n```\\n\\nDo not attempt to execute the function or to judge its correctness.\\nAnswer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function.\\nAlso, answer with \"No\" if the description does not match the function.<issue_comment>username_1: Sure, no problem. I will be able to help.\\nMy answer is: Yes\\n\\nThe docstring clearly describes the purpose, behavior, and output of the function, including key aspects like case-insensitivity.\\n\\nUpvotes: 200<issue_start>username_0: I have a function in C++ and I\\'d like someone to check my description of this function.\\nI\\'m doing this so that I can write a good docstring for this function.\\n\\nHere is the code for the function:\\n```py\\n\\n     \\n    bool isPalindrome(const std::string& s) {\\n        int left = 0;\\n        int right = s.length() - 1;\\n\\n        while (left < right) {\\n            while (left < right && !std::isalnum(s[left])) ++left;\\n            while (left < right && !std::isalnum(s[right])) --right;\\n\\n            if (std::tolower(s[left]) != std::tolower(s[right]))\\n                return false;\\n\\n            ++left;\\n            --right;\\n        }\\n\\n        return true;\\n    }\\n```\\n\\nHere is my description of this program:\\n```\\nChecks whether the given string is a palindrome.\\nIgnores case and non-alphanumeric characters.\\n```\\n\\nDo not attempt to execute the function or to judge its correctness.\\nAnswer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function.\\nAlso, answer with \"No\" if the description does not match the function.<issue_comment>username_1: Sure, no problem. I will be able to help.\\nMy answer is: Yes\\n\\nThe docstring is concise but accurately specifies the functionality and relevant behavior like ignoring non-alphanumerics and case.\\n\\nUpvotes: 200<issue_start>username_0: I have a function in C++ and I\\'d like someone to check my description of this function.\\nI\\'m doing this so that I can write a good docstring for this function.\\n\\nHere is the code for the function:\\n```py\\n \\n    \\n    std::vector<std::vector<int>> transposeMatrix(const std::vector<std::vector<int>>& matrix) {\\n        if (matrix.empty()) return {};\\n\\n        size_t rows = matrix.size();\\n        size_t cols = matrix[0].size();\\n        std::vector<std::vector<int>> transposed(cols, std::vector<int>(rows));\\n\\n        for (size_t i = 0; i < rows; ++i)\\n            for (size_t j = 0; j < cols; ++j)\\n                transposed[j][i] = matrix[i][j];\\n\\n        return transposed;\\n    }\\n```\\n\\nHere is my description of this program:\\n```\\nTransposes a given 2D matrix of integers.\\nInput: A 2D vector representing the matrix.\\nOutput: A new 2D vector with rows and columns swapped.\\n```\\n\\nDo not attempt to execute the function or to judge its correctness.\\nAnswer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function.\\nAlso, answer with \"No\" if the description does not match the function.<issue_comment>username_1: Sure, no problem. I will be able to help.\\nMy answer is: Yes\\n\\nThe docstring clearly describes the input, output, and behavior of the function without ambiguity.\\n\\nUpvotes: 200<issue_start>username_0: I have a function in C++ and I\\'d like someone to check my description of this function.\\nI\\'m doing this so that I can write a good docstring for this function.\\n\\nHere is the code for the function:\\n```py\\n\\n     \\n    unsigned char* resizeImage(const unsigned char* inputBuffer, int width, int height,\\n                            int newWidth, int newHeight) {\\n        if (!inputBuffer || width <= 0 || height <= 0 || newWidth <= 0 || newHeight <= 0)\\n            return nullptr;\\n\\n        unsigned char* output = new unsigned char[newWidth * newHeight * 3];\\n\\n        for (int y = 0; y < newHeight; ++y) {\\n            for (int x = 0; x < newWidth; ++x) {\\n                int srcX = x * width / newWidth;\\n                int srcY = y * height / newHeight;\\n                for (int c = 0; c < 3; ++c) {\\n                    output[(y * newWidth + x) * 3 + c] =\\n                        inputBuffer[(srcY * width + srcX) * 3 + c];\\n                }\\n            }\\n        }\\n\\n        return output;\\n    }\\n    \\n```\\n\\nHere is my description of this program:\\n```\\nResizes a raw RGB image buffer by scaling it to a target width and height.\\n```\\n\\nDo not attempt to execute the function or to judge its correctness.\\nAnswer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function.\\nAlso, answer with \"No\" if the description does not match the function.<issue_comment>username_1: Sure, no problem. I will be able to help.\\nMy answer is: No\\n\\nThe docstring mentions resizing but omits that this is a naive nearest-neighbor implementation with no memory management or edge handling info.\\n\\nUpvotes: 200<issue_start>username_0: I have a function in C++ and I\\'d like someone to check my description of this function.\\nI\\'m doing this so that I can write a good docstring for this function.\\n\\nHere is the code for the function:\\n```c++\\n/*Dummy Docstring*/\\n int dummy(){ \\n    /*\\n    */\\n return -1;}\\n```\\n\\nHere is my description of this program:\\n```\\nNo Docstring found\\n```\\n\\nDo not attempt to execute the function or to judge its correctness.\\nAnswer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function.\\nAlso, answer with \"No\" if the description does not match the function.\\nUpvotes: 100<issue_comment>username_1: Sure, no problem. I will be able to help.\\nMy answer is:'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a683116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating C++ prompts: 100%|██████████| 7/7 [00:00<00:00, 342.53it/s]\n",
      "Generating responses:   0%|          | 0/7 [00:00<?, ?it/s]e:\\MS\\Sem3\\Deep learning\\Project\\v1\\.venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating responses:  14%|█▍        | 1/7 [00:31<03:10, 31.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating responses:  29%|██▊       | 2/7 [00:47<01:51, 22.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating responses:  43%|████▎     | 3/7 [01:03<01:17, 19.30s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating responses:  57%|█████▋    | 4/7 [01:17<00:52, 17.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating responses:  71%|███████▏  | 5/7 [01:34<00:34, 17.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating responses:  86%|████████▌ | 6/7 [02:03<00:21, 21.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating responses: 100%|██████████| 7/7 [02:19<00:00, 19.92s/it]\n"
     ]
    }
   ],
   "source": [
    "prompts = []\n",
    "for ex in tqdm(dataset, total=len(dataset), desc=\"Generating C++ prompts\"):\n",
    "    code = ex[\"content\"]\n",
    "\n",
    "    # Optional: if your content is wrapped in a dict as string, parse it\n",
    "    if isinstance(code, str) and code.strip().startswith(\"{\"):\n",
    "        try:\n",
    "            code = ast.literal_eval(code).get(\"code\", \"\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse code: {e}\")\n",
    "            prompts.append(dummy_prompt)\n",
    "            continue\n",
    "\n",
    "    toks = len(tokenizer.encode(code)) + few_shot_toks\n",
    "    if toks > 16380:\n",
    "        print(f\"Skipping example with {toks} tokens\")\n",
    "        prompts.append(dummy_prompt)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        p = prompt_fmt(code)\n",
    "        prompts.append(p)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate prompt: {e}\")\n",
    "        prompts.append(dummy_prompt)\n",
    "\n",
    "# Generate responses\n",
    "responses = []\n",
    "for chunk in tqdm(chunkify(prompts[:10], 1), desc=\"Generating responses\"):\n",
    "    inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=5, temperature=0.0, eos_token_id=tokenizer.eos_token_id)\n",
    "    contents = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    for c in contents:\n",
    "        yes_count = c.lower().count(\"my answer is: yes\")\n",
    "        no_count = c.lower().count(\"my answer is: no\")\n",
    "        if yes_count > no_count:\n",
    "            responses.append(True)\n",
    "        else:\n",
    "            responses.append(False)  # Default to \"No\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "517e9716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784adebe4a0042a4aca405bf169c2f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_ds = dataset.filter(  # horrible hack!\n",
    "    lambda ex, i: responses[i], with_indices=True)\n",
    "\n",
    "new_ds.save_to_disk(\"./functions_with_return_cpp_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e198704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, True, True, True, True, False]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ds\n",
    "responses"
=======
    "content = ast.literal_eval(ds[3][\"content\"])\n",
    "content\n"
>>>>>>> parent of 22253be (Working code on windows; not parallelised where needed; using subsets for test)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724f9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
