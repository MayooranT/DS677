{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805e9d4c-f390-4aba-9ec8-627b9ed85aea",
   "metadata": {},
   "source": [
    "### SEED GATHERING GET CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a752eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++++++++++++++ Run this first time if you haven't installed from requirements.txt file/cloned the repo+++++++++++++++++++++\n",
    "# !pip install tree-sitter==0.20.4\n",
    "# !git clone https://github.com/tree-sitter/tree-sitter-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd3e144-73cb-42fa-9550-075c51686a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter_parser import LANGUAGE, make_parser, node_to_string\n",
    "import datasets\n",
    "import os\n",
    "import signal\n",
    "from multiprocessing import Pool\n",
    "#import os\n",
    "import boto3\n",
    "import smart_open\n",
    "#from datasets import load_dataset,Dataset\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "def download_contents(blob_id, src_encoding):\n",
    "    s3_url = f\"s3://softwareheritage/content/{blob_id}\"\n",
    "    with smart_open.open(s3_url, \"rb\", compression=\".gz\", transport_params={\"client\": s3}) as fin:\n",
    "        content = fin.read().decode(src_encoding)\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c8ee925-5001-4a30-b29f-5741f8d173a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPLEVEL_DOC_COMMENT_QUERY = LANGUAGE.query(\"\"\"\n",
    "(\n",
    "  (function_definition\n",
    "    declarator: (function_declarator\n",
    "      declarator: (identifier) @fn-name\n",
    "    )\n",
    "    body: (compound_statement\n",
    "      (comment) @doc.comment\n",
    "    )\n",
    "  ) @function.def\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "'''\n",
    "def get_fns_with_docstrings(src, tree):\n",
    "    captures = TOPLEVEL_DOC_COMMENT_QUERY.captures(tree.root_node)\n",
    "    res = []\n",
    "    for capture in captures:\n",
    "        node, ty = capture\n",
    "        if ty != \"function.def\":\n",
    "            continue\n",
    "        # if the starting col is not 0, then it's not a top-level fn\n",
    "        _, col = node.start_point\n",
    "        if col != 0:\n",
    "            continue\n",
    "        res.append(node_to_string(src, node))\n",
    "    return res\n",
    "'''\n",
    "\n",
    "def get_fns_with_docstrings(src, tree):\n",
    "    captures = TOPLEVEL_DOC_COMMENT_QUERY.captures(tree.root_node)\n",
    "    res = []\n",
    "    current = {\"function_node\": None, \"name\": None, \"doc\": None}\n",
    "\n",
    "    for node, capture_name in captures:\n",
    "        if capture_name == \"fn-name\":\n",
    "            current[\"name\"] = node_to_string(src, node)\n",
    "\n",
    "        elif capture_name == \"doc.comment\":\n",
    "            current[\"doc\"] = node_to_string(src, node)\n",
    "\n",
    "        elif capture_name == \"function.def\":\n",
    "            current[\"function_node\"] = node\n",
    "\n",
    "            # Build the result once we have everything\n",
    "            if current[\"name\"] and current[\"doc\"]:\n",
    "                full_func_text = node_to_string(src, current[\"function_node\"])\n",
    "                res.append({\n",
    "                    \"function_name\": current[\"name\"],\n",
    "                    \"docstring\": current[\"doc\"],\n",
    "                    \"code\": full_func_text\n",
    "                })\n",
    "\n",
    "            # Reset for next\n",
    "            current = {\"function_node\": None, \"name\": None, \"doc\": None}\n",
    "\n",
    "    return res\n",
    "\n",
    "def parse_ex(parser, ex):\n",
    "    #ex = ex[\"content\"]\n",
    "    ex = download_contents(ex[\"blob_id\"], ex[\"src_encoding\"])\n",
    "    try:\n",
    "        buf = bytes(ex, \"utf8\")\n",
    "        tree = parser.parse(buf)\n",
    "        return get_fns_with_docstrings(buf, tree)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "# if one parser segfaults, we can just make a new one and other parsers will still be fine\n",
    "# WE LOVE TREE SITTER!\n",
    "PARSERS = None\n",
    "\n",
    "\n",
    "def process_chunk(idx_and_chunk):\n",
    "    assert PARSERS is not None\n",
    "    idx, chunk = idx_and_chunk\n",
    "    parser = PARSERS[idx]\n",
    "    chunk_new_funs = set()\n",
    "    for ex in chunk:\n",
    "        chunk_new_funs.update(parse_ex(parser, ex))\n",
    "    return chunk_new_funs\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    global PARSERS\n",
    "    ds = datasets.load_dataset(\n",
    "        args.dataset,\n",
    "        data_dir=args.data_dir,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    funs = set()\n",
    "    PARSERS = [make_parser() for _ in range(args.num_workers)]\n",
    "    total_len = len(ds)\n",
    "    CHUNK_SIZE = 1000 * args.num_workers\n",
    "\n",
    "    print(f\"Total length: {total_len}\")\n",
    "    print(f\"Chunk size: {CHUNK_SIZE}\")\n",
    "\n",
    "    chunk = []\n",
    "    p = Pool(args.num_workers)\n",
    "    for i, ex in enumerate(ds):\n",
    "        if i % (total_len // 100) == 0:\n",
    "            print(f\"{i}/{total_len}\")\n",
    "        try:\n",
    "            chunk.append(ex)\n",
    "            if len(chunk) == CHUNK_SIZE or i == total_len - 1:\n",
    "                print(f\"Processing chunk {i // CHUNK_SIZE}\")\n",
    "                # divide the chunk into NUM_WORKERS chunks\n",
    "                subchunk_size = len(chunk) // args.num_workers\n",
    "                subchunks = [chunk[i:i + subchunk_size]\n",
    "                             for i in range(0, len(chunk), subchunk_size)]\n",
    "                new_funs_iter = p.imap(\n",
    "                    process_chunk, [(i, subchunk) for i, subchunk in enumerate(subchunks)])\n",
    "                print(\"Getting new functions\")\n",
    "                len_before = len(funs)\n",
    "                while True:\n",
    "                    try:\n",
    "                        def timeout_handler(_, __):\n",
    "                            raise KeyboardInterrupt  # it's fineeeeeee\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(60)\n",
    "                        funs.update(next(new_funs_iter))\n",
    "                        signal.alarm(0)\n",
    "                    except KeyboardInterrupt:\n",
    "                        signal.alarm(0)\n",
    "                        print(\"Keyboard interrupt. Terminating pool\")\n",
    "                        p.terminate()\n",
    "                        p = Pool(args.num_workers)\n",
    "                        break\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                signal.alarm(0)\n",
    "\n",
    "                PARSERS = [make_parser() for _ in range(args.num_workers)]\n",
    "\n",
    "                print(\n",
    "                    f\"Done processing chunk {i // CHUNK_SIZE}. Got {len(funs) - len_before} new functions\")\n",
    "\n",
    "                chunk = []\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            chunk = []\n",
    "\n",
    "        if i == total_len - 1:\n",
    "            break\n",
    "\n",
    "    p.close()\n",
    "\n",
    "    new_ds_dict = {\n",
    "        \"content\": list(funs),\n",
    "        \"id\": list(range(len(funs)))\n",
    "    }\n",
    "\n",
    "    new_ds = datasets.Dataset.from_dict(new_ds_dict)\n",
    "    #new_ds.push_to_hub(args.push, private=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74accea3-de2a-4b38-bbf2-b0c7f2be7aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMWORKERS = os.cpu_count()\n",
    "NUMWORKERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8798ed1-24c7-4694-a97a-a381ab122392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1332eec6916b44568331e600521ffde5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"bigcode/the-stack-v2-dedup\", \"C++\", cache_dir=f\"./cache/stack\", streaming=False, split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6095d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import islice\n",
    "\n",
    "# small_subset = islice(ds, 50)\n",
    "\n",
    "# # Convert to list if you want to materialize it (use with caution, as this loads into memory)\n",
    "# ds = list(small_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b23a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted functions in 0 and doc-comments:\n",
      "Extracted functions in 1 and doc-comments:\n",
      "Extracted functions in 2 and doc-comments:\n",
      "Extracted functions in 3 and doc-comments:\n",
      "Extracted functions in 4 and doc-comments:\n"
     ]
    }
   ],
   "source": [
    "# Setup a single Parser\n",
    "funs = set()\n",
    "\n",
    "parser = make_parser()\n",
    "\n",
    "for i in range(5):\n",
    "# Take one example manually\n",
    "    ex = ds[i]  # First example (directly)\n",
    "\n",
    "    # Download content if needed\n",
    "    content = download_contents(ex[\"blob_id\"], ex[\"src_encoding\"])\n",
    "\n",
    "    # Parse\n",
    "    src = bytes(content, \"utf8\")\n",
    "    tree = parser.parse(src)\n",
    "\n",
    "    # Extract functions\n",
    "    functions = get_fns_with_docstrings(src, tree)\n",
    "    #funs.update(functions)\n",
    "    # Print results\n",
    "    print(f\"Extracted functions in {i} and doc-comments:\")\n",
    "    for fn in functions:\n",
    "        print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9a1930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 1000\n",
      "0/1000\n",
      "\n",
      "Processing chunk 0...\n",
      "✅ Done chunk 0. Got 14 new functions.\n",
      "10/1000\n",
      "\n",
      "Processing chunk 1...\n",
      "✅ Done chunk 1. Got 0 new functions.\n",
      "20/1000\n",
      "\n",
      "Processing chunk 2...\n",
      "✅ Done chunk 2. Got 1 new functions.\n",
      "30/1000\n",
      "\n",
      "Processing chunk 3...\n",
      "✅ Done chunk 3. Got 0 new functions.\n",
      "40/1000\n",
      "\n",
      "Processing chunk 4...\n",
      "✅ Done chunk 4. Got 4 new functions.\n",
      "50/1000\n",
      "\n",
      "Processing chunk 5...\n",
      "✅ Done chunk 5. Got 1 new functions.\n",
      "60/1000\n",
      "\n",
      "Processing chunk 6...\n",
      "✅ Done chunk 6. Got 0 new functions.\n",
      "70/1000\n",
      "\n",
      "Processing chunk 7...\n",
      "✅ Done chunk 7. Got 10 new functions.\n",
      "80/1000\n",
      "\n",
      "Processing chunk 8...\n",
      "✅ Done chunk 8. Got 3 new functions.\n",
      "90/1000\n",
      "\n",
      "Processing chunk 9...\n",
      "✅ Done chunk 9. Got 0 new functions.\n",
      "100/1000\n",
      "\n",
      "Processing chunk 10...\n",
      "✅ Done chunk 10. Got 0 new functions.\n",
      "110/1000\n",
      "\n",
      "Processing chunk 11...\n",
      "✅ Done chunk 11. Got 0 new functions.\n",
      "120/1000\n",
      "\n",
      "Processing chunk 12...\n",
      "✅ Done chunk 12. Got 0 new functions.\n",
      "130/1000\n",
      "\n",
      "Processing chunk 13...\n",
      "✅ Done chunk 13. Got 0 new functions.\n",
      "140/1000\n",
      "\n",
      "Processing chunk 14...\n",
      "✅ Done chunk 14. Got 0 new functions.\n",
      "150/1000\n",
      "\n",
      "Processing chunk 15...\n",
      "✅ Done chunk 15. Got 2 new functions.\n",
      "160/1000\n",
      "\n",
      "Processing chunk 16...\n",
      "✅ Done chunk 16. Got 5 new functions.\n",
      "170/1000\n",
      "\n",
      "Processing chunk 17...\n",
      "✅ Done chunk 17. Got 1 new functions.\n",
      "180/1000\n",
      "\n",
      "Processing chunk 18...\n",
      "✅ Done chunk 18. Got 4 new functions.\n",
      "190/1000\n",
      "\n",
      "Processing chunk 19...\n",
      "✅ Done chunk 19. Got 0 new functions.\n",
      "200/1000\n",
      "\n",
      "Processing chunk 20...\n",
      "✅ Done chunk 20. Got 0 new functions.\n",
      "210/1000\n",
      "\n",
      "Processing chunk 21...\n",
      "✅ Done chunk 21. Got 0 new functions.\n",
      "220/1000\n",
      "\n",
      "Processing chunk 22...\n",
      "✅ Done chunk 22. Got 2 new functions.\n",
      "230/1000\n",
      "\n",
      "Processing chunk 23...\n",
      "✅ Done chunk 23. Got 1 new functions.\n",
      "240/1000\n",
      "\n",
      "Processing chunk 24...\n",
      "✅ Done chunk 24. Got 0 new functions.\n",
      "250/1000\n",
      "\n",
      "Processing chunk 25...\n",
      "✅ Done chunk 25. Got 4 new functions.\n",
      "260/1000\n",
      "\n",
      "Processing chunk 26...\n",
      "✅ Done chunk 26. Got 2 new functions.\n",
      "270/1000\n",
      "\n",
      "Processing chunk 27...\n",
      "✅ Done chunk 27. Got 22 new functions.\n",
      "280/1000\n",
      "\n",
      "Processing chunk 28...\n",
      "✅ Done chunk 28. Got 1 new functions.\n",
      "290/1000\n",
      "\n",
      "Processing chunk 29...\n",
      "✅ Done chunk 29. Got 0 new functions.\n",
      "300/1000\n",
      "\n",
      "Processing chunk 30...\n",
      "✅ Done chunk 30. Got 0 new functions.\n",
      "310/1000\n",
      "\n",
      "Processing chunk 31...\n",
      "✅ Done chunk 31. Got 0 new functions.\n",
      "320/1000\n",
      "\n",
      "Processing chunk 32...\n",
      "✅ Done chunk 32. Got 1 new functions.\n",
      "330/1000\n",
      "\n",
      "Processing chunk 33...\n",
      "✅ Done chunk 33. Got 0 new functions.\n",
      "340/1000\n",
      "\n",
      "Processing chunk 34...\n",
      "✅ Done chunk 34. Got 0 new functions.\n",
      "350/1000\n",
      "\n",
      "Processing chunk 35...\n",
      "✅ Done chunk 35. Got 0 new functions.\n",
      "360/1000\n",
      "\n",
      "Processing chunk 36...\n",
      "✅ Done chunk 36. Got 1 new functions.\n",
      "370/1000\n",
      "\n",
      "Processing chunk 37...\n",
      "✅ Done chunk 37. Got 5 new functions.\n",
      "380/1000\n",
      "\n",
      "Processing chunk 38...\n",
      "✅ Done chunk 38. Got 6 new functions.\n",
      "390/1000\n",
      "\n",
      "Processing chunk 39...\n",
      "✅ Done chunk 39. Got 14 new functions.\n",
      "400/1000\n",
      "\n",
      "Processing chunk 40...\n",
      "✅ Done chunk 40. Got 0 new functions.\n",
      "410/1000\n",
      "\n",
      "Processing chunk 41...\n",
      "✅ Done chunk 41. Got 1 new functions.\n",
      "420/1000\n",
      "\n",
      "Processing chunk 42...\n",
      "✅ Done chunk 42. Got 1 new functions.\n",
      "430/1000\n",
      "\n",
      "Processing chunk 43...\n",
      "✅ Done chunk 43. Got 1 new functions.\n",
      "440/1000\n",
      "\n",
      "Processing chunk 44...\n",
      "✅ Done chunk 44. Got 0 new functions.\n",
      "450/1000\n",
      "\n",
      "Processing chunk 45...\n",
      "✅ Done chunk 45. Got 0 new functions.\n",
      "460/1000\n",
      "\n",
      "Processing chunk 46...\n",
      "✅ Done chunk 46. Got 4 new functions.\n",
      "470/1000\n",
      "\n",
      "Processing chunk 47...\n",
      "✅ Done chunk 47. Got 0 new functions.\n",
      "480/1000\n",
      "\n",
      "Processing chunk 48...\n",
      "✅ Done chunk 48. Got 11 new functions.\n",
      "490/1000\n",
      "\n",
      "Processing chunk 49...\n",
      "✅ Done chunk 49. Got 0 new functions.\n",
      "500/1000\n",
      "\n",
      "Processing chunk 50...\n",
      "✅ Done chunk 50. Got 1 new functions.\n",
      "510/1000\n",
      "\n",
      "Processing chunk 51...\n",
      "✅ Done chunk 51. Got 0 new functions.\n",
      "520/1000\n",
      "\n",
      "Processing chunk 52...\n",
      "✅ Done chunk 52. Got 3 new functions.\n",
      "530/1000\n",
      "\n",
      "Processing chunk 53...\n",
      "✅ Done chunk 53. Got 5 new functions.\n",
      "540/1000\n",
      "\n",
      "Processing chunk 54...\n",
      "✅ Done chunk 54. Got 1 new functions.\n",
      "550/1000\n",
      "\n",
      "Processing chunk 55...\n",
      "✅ Done chunk 55. Got 0 new functions.\n",
      "560/1000\n",
      "\n",
      "Processing chunk 56...\n",
      "✅ Done chunk 56. Got 0 new functions.\n",
      "570/1000\n",
      "\n",
      "Processing chunk 57...\n",
      "✅ Done chunk 57. Got 0 new functions.\n",
      "580/1000\n",
      "\n",
      "Processing chunk 58...\n",
      "✅ Done chunk 58. Got 0 new functions.\n",
      "590/1000\n",
      "\n",
      "Processing chunk 59...\n",
      "✅ Done chunk 59. Got 0 new functions.\n",
      "600/1000\n",
      "\n",
      "Processing chunk 60...\n",
      "✅ Done chunk 60. Got 2 new functions.\n",
      "610/1000\n",
      "\n",
      "Processing chunk 61...\n",
      "✅ Done chunk 61. Got 3 new functions.\n",
      "620/1000\n",
      "\n",
      "Processing chunk 62...\n",
      "✅ Done chunk 62. Got 0 new functions.\n",
      "630/1000\n",
      "\n",
      "Processing chunk 63...\n",
      "✅ Done chunk 63. Got 1 new functions.\n",
      "640/1000\n",
      "\n",
      "Processing chunk 64...\n",
      "✅ Done chunk 64. Got 0 new functions.\n",
      "650/1000\n",
      "\n",
      "Processing chunk 65...\n",
      "✅ Done chunk 65. Got 3 new functions.\n",
      "660/1000\n",
      "\n",
      "Processing chunk 66...\n",
      "✅ Done chunk 66. Got 1 new functions.\n",
      "670/1000\n",
      "\n",
      "Processing chunk 67...\n",
      "✅ Done chunk 67. Got 2 new functions.\n",
      "680/1000\n",
      "\n",
      "Processing chunk 68...\n",
      "✅ Done chunk 68. Got 1 new functions.\n",
      "690/1000\n",
      "\n",
      "Processing chunk 69...\n",
      "✅ Done chunk 69. Got 6 new functions.\n",
      "700/1000\n",
      "\n",
      "Processing chunk 70...\n",
      "✅ Done chunk 70. Got 0 new functions.\n",
      "710/1000\n",
      "\n",
      "Processing chunk 71...\n",
      "✅ Done chunk 71. Got 1 new functions.\n",
      "720/1000\n",
      "\n",
      "Processing chunk 72...\n",
      "✅ Done chunk 72. Got 0 new functions.\n",
      "730/1000\n",
      "\n",
      "Processing chunk 73...\n",
      "✅ Done chunk 73. Got 0 new functions.\n",
      "740/1000\n",
      "\n",
      "Processing chunk 74...\n",
      "✅ Done chunk 74. Got 0 new functions.\n",
      "750/1000\n",
      "\n",
      "Processing chunk 75...\n",
      "✅ Done chunk 75. Got 0 new functions.\n",
      "760/1000\n",
      "\n",
      "Processing chunk 76...\n",
      "✅ Done chunk 76. Got 1 new functions.\n",
      "770/1000\n",
      "\n",
      "Processing chunk 77...\n",
      "✅ Done chunk 77. Got 5 new functions.\n",
      "780/1000\n",
      "\n",
      "Processing chunk 78...\n",
      "✅ Done chunk 78. Got 4 new functions.\n",
      "790/1000\n",
      "\n",
      "Processing chunk 79...\n",
      "✅ Done chunk 79. Got 0 new functions.\n",
      "800/1000\n",
      "\n",
      "Processing chunk 80...\n",
      "✅ Done chunk 80. Got 0 new functions.\n",
      "810/1000\n",
      "\n",
      "Processing chunk 81...\n",
      "✅ Done chunk 81. Got 1 new functions.\n",
      "820/1000\n",
      "\n",
      "Processing chunk 82...\n",
      "✅ Done chunk 82. Got 0 new functions.\n",
      "830/1000\n",
      "\n",
      "Processing chunk 83...\n",
      "✅ Done chunk 83. Got 8 new functions.\n",
      "840/1000\n",
      "\n",
      "Processing chunk 84...\n",
      "✅ Done chunk 84. Got 2 new functions.\n",
      "850/1000\n",
      "\n",
      "Processing chunk 85...\n",
      "✅ Done chunk 85. Got 0 new functions.\n",
      "860/1000\n",
      "\n",
      "Processing chunk 86...\n",
      "✅ Done chunk 86. Got 0 new functions.\n",
      "870/1000\n",
      "\n",
      "Processing chunk 87...\n",
      "✅ Done chunk 87. Got 5 new functions.\n",
      "880/1000\n",
      "\n",
      "Processing chunk 88...\n",
      "✅ Done chunk 88. Got 1 new functions.\n",
      "890/1000\n",
      "\n",
      "Processing chunk 89...\n",
      "✅ Done chunk 89. Got 0 new functions.\n",
      "900/1000\n",
      "\n",
      "Processing chunk 90...\n",
      "✅ Done chunk 90. Got 0 new functions.\n",
      "910/1000\n",
      "\n",
      "Processing chunk 91...\n",
      "✅ Done chunk 91. Got 0 new functions.\n",
      "920/1000\n",
      "\n",
      "Processing chunk 92...\n",
      "✅ Done chunk 92. Got 0 new functions.\n",
      "930/1000\n",
      "\n",
      "Processing chunk 93...\n",
      "✅ Done chunk 93. Got 6 new functions.\n",
      "940/1000\n",
      "\n",
      "Processing chunk 94...\n",
      "✅ Done chunk 94. Got 14 new functions.\n",
      "950/1000\n",
      "\n",
      "Processing chunk 95...\n",
      "✅ Done chunk 95. Got 0 new functions.\n",
      "960/1000\n",
      "\n",
      "Processing chunk 96...\n",
      "✅ Done chunk 96. Got 4 new functions.\n",
      "970/1000\n",
      "\n",
      "Processing chunk 97...\n",
      "✅ Done chunk 97. Got 3 new functions.\n",
      "980/1000\n",
      "\n",
      "Processing chunk 98...\n",
      "✅ Done chunk 98. Got 4 new functions.\n",
      "990/1000\n",
      "\n",
      "Processing chunk 99...\n",
      "✅ Done chunk 99. Got 0 new functions.\n",
      "\n",
      "Total unique functions collected: 210\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "def process_chunk(idx_and_chunk):\n",
    "    assert PARSERS is not None\n",
    "    idx, chunk = idx_and_chunk\n",
    "    parser = PARSERS[idx]\n",
    "    chunk_new_funs = set()\n",
    "    \n",
    "    for ex in chunk:\n",
    "        functions = parse_ex(parser, ex)\n",
    "        for fn in functions:\n",
    "            chunk_new_funs.add(str(fn))  # <=== Fix here\n",
    "\n",
    "    return chunk_new_funs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUMWORKERS = 1\n",
    "CHUNK_SIZE = 10  # Adjust if needed\n",
    "\n",
    "PARSERS = [make_parser() for _ in range(NUMWORKERS)]\n",
    "\n",
    "funs = set()\n",
    "chunk = []\n",
    "\n",
    "total_len = len(ds)\n",
    "\n",
    "print(f\"Total dataset size: {total_len}\")\n",
    "\n",
    "# Loop over dataset\n",
    "for i, ex in enumerate(iter(ds)):\n",
    "    if i % (max(total_len // 100, 1)) == 0:\n",
    "        print(f\"{i}/{total_len}\")\n",
    "\n",
    "    chunk.append(ex)\n",
    "\n",
    "    if len(chunk) >= CHUNK_SIZE or i == total_len - 1:\n",
    "        print(f\"\\nProcessing chunk {i // CHUNK_SIZE}...\")\n",
    "\n",
    "        # Split the chunk into subchunks\n",
    "        subchunk_size = max(1, len(chunk) // NUMWORKERS)\n",
    "        subchunks = [chunk[j:j + subchunk_size] for j in range(0, len(chunk), subchunk_size)]\n",
    "\n",
    "        len_before = len(funs)\n",
    "\n",
    "        # Sequentially process each subchunk using process_chunk\n",
    "        for idx, subchunk in enumerate(subchunks):\n",
    "            chunk_funs = process_chunk((idx, subchunk))\n",
    "            funs.update(chunk_funs)\n",
    "\n",
    "        print(f\"✅ Done chunk {i // CHUNK_SIZE}. Got {len(funs) - len_before} new functions.\")\n",
    "\n",
    "        chunk = []  # Reset chunk\n",
    "        PARSERS = [make_parser() for _ in range(NUMWORKERS)]  # Rebuild parsers if needed\n",
    "\n",
    "# Final dataset creation\n",
    "print(f\"\\nTotal unique functions collected: {len(funs)}\")\n",
    "\n",
    "new_ds_dict = {\n",
    "    \"content\": list(funs),\n",
    "    \"id\": list(range(len(funs)))\n",
    "}\n",
    "\n",
    "new_ds = datasets.Dataset.from_dict(new_ds_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ce4e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2055e71c45e54eb6a0db02b588faa741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'content': \"{'function_name': 'Max', 'docstring': '//求n个数的max ', 'code': 'void push_down(Splay *x){//下放标记 \\\\n\\\\tif(x==null)return;\\\\n\\\\tif(x->rev){//区间翻转 \\\\n\\\\t\\\\tx->rev=0,x->son[0]->rev^=1,x->son[1]->rev^=1;\\\\n\\\\t\\\\tswap(x->son[0],x->son[1]),swap(x->maxl,x->maxr);\\\\n\\\\t}\\\\n\\\\tif(x->same){//区间赋值 \\\\n\\\\t\\\\tx->same=0,x->son[0]->key=x->son[1]->key=x->key;\\\\n\\\\t\\\\tx->son[0]->same=x->son[1]->same=1;\\\\n\\\\t\\\\tx->sum=x->key*x->size;\\\\n\\\\t\\\\tx->maxl=x->maxr=x->maxt=(x->key>0?x->sum:x->key);\\\\n\\\\t}\\\\n}'}\",\n",
       " 'id': 7}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ds.save_to_disk(\"./extracted_functions_cpp\")\n",
    "new_ds[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086c5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"./extracted_functions_cpp\")\n",
    "\n",
    "# for example in ds:\n",
    "#     print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6741a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "from tree_sitter_parser import LANGUAGE, make_parser, does_have_return\n",
    "\n",
    "# Define return-statement query for C++\n",
    "RETURN_QUERY = LANGUAGE.query(\"\"\"\n",
    "(return_statement) @return\n",
    "\"\"\")\n",
    "\n",
    "# Use a Tree-sitter parser set to C++\n",
    "parser = make_parser()\n",
    "\n",
    "# Filter dataset to only functions with meaningful return values\n",
    "def filter_cpp_functions_with_return(ds: Dataset) -> Dataset:\n",
    "    filtered_ds = []\n",
    "    for i in ds:\n",
    "        if does_have_return(i[\"content\"], parser):\n",
    "            filtered_ds.append(i)\n",
    "    return filtered_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa5a69f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering to only C++ functions with return statements...\n",
      "✅ Filtered dataset size: 73\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering to only C++ functions with return statements...\")\n",
    "filtered_ds = filter_cpp_functions_with_return(ds)\n",
    "print(f\"✅ Filtered dataset size: {len(filtered_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eec27f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f172b032e9447eb3967dd782e2323e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/73 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert the list into a Dataset\n",
    "filtered_ds = Dataset.from_list(filtered_ds)\n",
    "\n",
    "# Now you can save it to disk\n",
    "filtered_ds.save_to_disk(\"./functions_with_return_cpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e55f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to filter out functions that have valid types is next but I'm not sure we need it for c++ as functions are already typed\n",
    "#Should check with TA if it is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "133ec49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"./functions_with_return_cpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83e81424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_name': 'loop',\n",
       " 'docstring': '/*\\n    //MOSTRAR INFO EN LED\\n    if (isDoorOpen==true) {\\n      ledToggle(\"on\");\\n    } else {\\n      ledToggle(\"off\");\\n    }\\n\\n    //MOSTRAR INFO EN LED\\n    if (hasDetection==true) {\\n      ledToggle(\"on\");\\n    } else {\\n      ledToggle(\"off\");\\n    }\\n*/',\n",
       " 'code': 'int setColor(String command)    {\\n    // Look through the list of colors to find the one that was requested\\n    for(int iColor = 0; iColor < NUM_COLORS; iColor++)\\n    {\\n        if(command == colorName[iColor]) {\\n            // When it matches, look up the RGB values for that color in the table,\\n            // and write the red, green, and blue values.\\n            RGB.control(true);\\n            RGB.color(colorRGB[iColor][0], colorRGB[iColor][1], colorRGB[iColor][2]);\\n\\n            analogWrite(pinRed,colorRGB[iColor][0]);\\n            analogWrite(pinGreen,colorRGB[iColor][1]);\\n            analogWrite(pinBlue,colorRGB[iColor][2]);\\n            return 0;\\n        }\\n    }\\n\\n    return -1;\\n}'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "content = ast.literal_eval(ds[3][\"content\"])\n",
    "content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724f9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
